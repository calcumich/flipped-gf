{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract frames from a video file\n",
    "def extract_frames(video_path, interval=1):\n",
    "    # Read the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Frame Per Second\n",
    "    frames = []\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return frames\n",
    "\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert frame to RGB (OpenCV uses BGR)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Save frame every 'interval' seconds\n",
    "            if frame_index % int(fps * interval) == 0:\n",
    "                frames.append(frame_rgb)\n",
    "            frame_index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract embeddings from a frame using CLIP ViT-L/14\n",
    "def extract_clip_embeddings(image):\n",
    "    # Load the CLIP model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "    # Preprocess the image and compute the features\n",
    "    image_preprocessed = preprocess(Image.fromarray(image)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_preprocessed)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    return image_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to extract embeddings from all frames of a video\n",
    "def extract_video_embeddings(video_path, interval=1):\n",
    "    frames = extract_frames(video_path, interval=interval)\n",
    "    embeddings = []\n",
    "    for frame in frames:\n",
    "        embedding = extract_clip_embeddings(frame)\n",
    "        embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)  # Stack embeddings of all frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@203.715] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (2386) handleMessage OpenCV | GStreamer warning: your GStreamer installation is missing a required plugin\n",
      "[ WARN:0@203.715] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (2402) handleMessage OpenCV | GStreamer warning: Embedded video playback halted; module uridecodebin1 reported: Your GStreamer installation is missing a plug-in.\n",
      "[ WARN:0@203.715] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (1356) open OpenCV | GStreamer warning: unable to start pipeline\n",
      "[ WARN:0@203.715] global /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_11nitadzeg/croot/opencv-suite_1691620374638/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (11, 768)\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "video_path = 'Interaction_T1_2297.mp4'  # Replace with your video path\n",
    "embeddings = extract_video_embeddings(video_path, interval=0.5)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import argparse, time\n",
    "from dataset.star import VideoQADataset, VideoQACollator, repeat_tensor_rows, trans_results\n",
    "from model.transformer_gf import build_transformer\n",
    "from model.glance_focus import GF, SetCriterion_UNS\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --device_id 0 --test_only 1 --qa_dataset star --base_data_dir dataset/STAR --reload_model_path expm/star/gf_logs/ckpts_2024-01-17T10-30-46/model_3000.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "args = Namespace(basedir='expm/star', name='gf_logs', device_id=0, batch_size=64, nepoch=10, lr=5e-06, i_val=300, i_test=300, i_print=100, i_weight=1000, test_only=1, reload_model_path='expm/star/gf_logs/ckpts_2024-01-17T10-30-46/model_3000.tar', hidden_dim=512, num_layers=2, num_queries=10, event_pred_dim=50, max_feats=80, qa_dataset='star', task_type='star', num_options=4, output_dim=1, base_data_dir='dataset/STAR', train_data_file_path='{}/txt_db/train.jsonl', test_data_file_path='{}/txt_db/test.jsonl', val_data_file_path='{}/txt_db/val.jsonl', event_anno_path='{}/txt_db/events.json', action_mapping_path='{}/txt_db/action_mapping.txt', app_feat_path='{}/vis_db/s3d.pth', feature_dim=1024, str2num_file='{}/vis_db/strID2numID.json', losses_type=['qa', 'cls', 'giou', 'cert'], qa_loss_coef=1, cls_loss_coef=0.5, giou_loss_coef=0.5, cert_loss_coef=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/nitin/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "transformer = build_transformer(args)\n",
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GF(\n",
    "        transformer,\n",
    "        num_queries=args.num_queries,\n",
    "        feature_dim=args.feature_dim,\n",
    "        output_dim=args.output_dim,\n",
    "        event_pred_dim=args.event_pred_dim,\n",
    "        qa_dataset=args.qa_dataset\n",
    "    ).to(device)\n",
    "\n",
    "# model = GF(\n",
    "#         transformer,\n",
    "#         num_queries= 10, # args.num_queries,\n",
    "#         feature_dim= 1024, # args.feature_dim,\n",
    "#         output_dim= 1, # args.output_dim,\n",
    "#         event_pred_dim= 50, # args.event_pred_dim,\n",
    "#         qa_dataset= 'star' # args.qa_dataset\n",
    "#     ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{       'vid': tensor([[0.3739, 0.5834, 0.1591,  ..., 0.2401, 0.0580, 0.5093],\n",
    "        [0.4297, 0.7157, 0.2008,  ..., 0.2973, 0.0579, 0.5230],\n",
    "        [0.5246, 0.6812, 0.2543,  ..., 0.2987, 0.0293, 0.4852],\n",
    "        ...,\n",
    "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]), \n",
    "        'examples': [{'q_str': 'Which object was put down by the person?', 'question_id': 'Interaction_T1_0', 'label': None, 'options_str_list': ['The food.', 'The laptop.', 'The book.', 'The pillow.']}], \n",
    "        'n_examples': 1, \n",
    "        'span': tensor([[0.7243, 1.0775],\n",
    "        [0.7782, 1.0775]]), \n",
    "        'hoi': tensor([61, 63])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0774748201438846"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18.0/16.705726819301727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Interaction_T1_0\": {\"duration\": 16.705726819301727, \"actions\": [[61, 12.1, 18.0], [63, 13.0, 18.0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(\n",
    "    vid=torch.Tensor(appearance_feat),\n",
    "    examples=examples,\n",
    "    n_examples=len(examples),  # used to create image feature copies.\n",
    "    span=span,\n",
    "    hoi=hoi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dict(\n",
    "    vid = torch.Tensor(embeddings),\n",
    "    examples = [{'q_str': 'Which object was thrown by the person?', 'question_id': 'Interaction_T1_2297', 'label': None, 'options_str_list': [\"The clothes.\", \"The bag.\", \"The blanket.\", \"The pillow.\"]}],\n",
    "    n_examples = 1,\n",
    "    span = torch.Tensor([[0.2620, 0.4627], [0.2620, 0.4422]]),\n",
    "    hoi = torch.Tensor([18, 16])\n",
    ")\n",
    "\n",
    "frame_features = collated_batch\n",
    "# {\"question_id\": \"Interaction_T1_2297\", \"question\": \"Which object was thrown by the person?\", \"video_id\": \"MIV2M\", \"options\": [\"The clothes.\", \"The bag.\", \"The blanket.\", \"The pillow.\"], \"answer\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_features = collated_batch['visual_inputs']\n",
    "frame_features = torch.stack(frame_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Interaction_T1_2297\": {\"duration\": 24.4244, \"actions\": [[18, 6.4, 11.3], [16, 6.4, 10.8]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (11x768 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m visual_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(frame_features\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m memory_cache \u001b[38;5;241m=\u001b[39m model(frame_features, visual_attention_mask, \u001b[38;5;28;01mNone\u001b[39;00m, encode_and_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, glance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m outputs_event \u001b[38;5;241m=\u001b[39m model(frame_features, visual_attention_mask, \u001b[38;5;28;01mNone\u001b[39;00m, encode_and_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, glance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,memory_cache\u001b[38;5;241m=\u001b[39mmemory_cache, query_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/W24/EECS 545 ML/glance_focus/Glance-Focus/model/glance_focus.py:89\u001b[0m, in \u001b[0;36mGF.forward\u001b[0;34m(self, src, mask, captions, encode_and_save, memory_cache, query_type, glance)\u001b[0m\n\u001b[1;32m     86\u001b[0m     src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([src, memory_prompt\u001b[38;5;241m+\u001b[39mposition_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# The visual input of glancing stage is frame only\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(src)\n\u001b[1;32m     90\u001b[0m memory_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m     91\u001b[0m     src,\n\u001b[1;32m     92\u001b[0m     mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     glance\u001b[38;5;241m=\u001b[39mglance\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m memory_cache\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/W24/EECS 545 ML/glance_focus/Glance-Focus/model/glance_focus.py:376\u001b[0m, in \u001b[0;36mFCNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (11x768 and 1024x512)"
     ]
    }
   ],
   "source": [
    "visual_attention_mask = torch.ones(frame_features.shape[:-1], dtype=torch.float).to(device)\n",
    "\n",
    "memory_cache = model(frame_features, visual_attention_mask, None, encode_and_save=True, glance=True)\n",
    "outputs_event = model(frame_features, visual_attention_mask, None, encode_and_save=False, glance=True,memory_cache=memory_cache, query_type='event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visual_inputs': [tensor([[-0.3643,  0.6107,  0.2408,  ..., -0.0514, -0.0760, -0.1956],\n",
      "        [-0.3247,  0.6130,  0.2897,  ...,  0.0068, -0.1226, -0.1965],\n",
      "        [-0.2733,  0.5206,  0.3079,  ..., -0.0090, -0.0889, -0.1642],\n",
      "        ...,\n",
      "        [ 0.0631, -0.2323,  0.0952,  ...,  0.1765, -0.5188, -0.2757],\n",
      "        [-0.1976, -0.2839,  0.3241,  ...,  0.1687, -0.2975, -0.2472],\n",
      "        [ 0.2986, -0.1343,  0.3143,  ...,  0.0101, -0.0746, -0.5556]])], 'text_str_list': ['Which object was thrown by the person? The clothes.', 'Which object was thrown by the person? The bag.', 'Which object was thrown by the person? The blanket.', 'Which object was thrown by the person? The pillow.'], 'question_ids': ['Interaction_T1_2297'], 'labels': None, 'n_examples_list': [1], 'span_lst': [tensor([[0.2620, 0.4627],\n",
      "        [0.2620, 0.4422]])], 'hoi_lst': [tensor([18., 16.])]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# Assuming the MULTI_CHOICE_QA contains 'star', adjust based on your case\n",
    "MULTI_CHOICE_QA = ['star', 'nextqa_mc']  # Add other types as necessary\n",
    "\n",
    "# Function to flatten a list of lists\n",
    "def flat_list_of_lists(lists):\n",
    "    return [item for sublist in lists for item in sublist]\n",
    "\n",
    "\n",
    "class VideoQACollator(object):\n",
    "    def __init__(self, task_type='star', n_options=4):\n",
    "        self.task_type = task_type\n",
    "        self.n_options = n_options\n",
    "        if self.task_type == 'nextqa_mc':\n",
    "            self.n_options = 5\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        visual_inputs = [d[\"vid\"] for d in batch]  # <list> (B, dict)\n",
    "        text_examples = flat_list_of_lists([d[\"examples\"] for d in batch])\n",
    "        n_examples_list = [d[\"n_examples\"] for d in batch]  # (B, )\n",
    "        if self.task_type in MULTI_CHOICE_QA:\n",
    "            text_str_list = flat_list_of_lists(\n",
    "                [[d[\"q_str\"] + \" \" + d[\"options_str_list\"][i] for i in range(self.n_options)]\n",
    "                 for d in text_examples]\n",
    "            )\n",
    "        else:\n",
    "            text_str_list = [d[\"q_str\"] for d in text_examples]\n",
    "        labels = default_collate([int(d[\"label\"]) for d in text_examples]) if text_examples[0][\"label\"] is not None else None\n",
    "        question_ids = [d[\"question_id\"] for d in text_examples]\n",
    "        span_lst = [d[\"span\"] for d in batch]\n",
    "        hoi_lst = [d[\"hoi\"] for d in batch]\n",
    "        return dict(\n",
    "            visual_inputs=visual_inputs,\n",
    "            text_str_list=text_str_list,\n",
    "            question_ids=question_ids,\n",
    "            labels=labels,\n",
    "            n_examples_list=n_examples_list,\n",
    "            span_lst=span_lst,\n",
    "            hoi_lst=hoi_lst\n",
    "        )\n",
    "\n",
    "\n",
    "collator = VideoQACollator(task_type='star', n_options=4)\n",
    "\n",
    "batch_data = [\n",
    "    {\n",
    "        'vid': torch.Tensor(embeddings),  # Embeddings should be defined elsewhere in your code\n",
    "        'examples': [\n",
    "            {\n",
    "                'q_str': 'Which object was thrown by the person?',\n",
    "                'question_id': 'Interaction_T1_2297',\n",
    "                'label': None,  # Replace with actual label if available\n",
    "                'options_str_list': [\"The clothes.\", \"The bag.\", \"The blanket.\", \"The pillow.\"]\n",
    "            }\n",
    "        ],\n",
    "        'n_examples': 1,\n",
    "        'span': torch.Tensor([[0.2620, 0.4627], [0.2620, 0.4422]]),\n",
    "        'hoi': torch.Tensor([18, 16])\n",
    "    }\n",
    "]\n",
    "\n",
    "collated_batch = collator.collate_batch(batch_data)\n",
    "print(collated_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
